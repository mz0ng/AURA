{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import anthropic\n",
    "import os\n",
    "import pickle\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from env import CellTowerEnv\n",
    "from agent import SingleAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power level to capacity mappings\n",
    "MAPPING1 = {\n",
    "        1: 25,\n",
    "        2: 35,\n",
    "        3: 43,\n",
    "        4: 50\n",
    "    }\n",
    "MAPPING2 = {\n",
    "        1: 8,\n",
    "        2: 11,\n",
    "        3: 14,\n",
    "        4: 17,\n",
    "        5: 21,\n",
    "        6: 25,\n",
    "        7: 28,\n",
    "        8: 30\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fb8f1",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ag1 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])\n",
    "llm_ag2 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_client = anthropic.Anthropic(api_key='')\n",
    "\n",
    "def claude_action(ag1_state, max_p1, ag2_state, max_p2):\n",
    "    prompt = f\"\"\"\n",
    "    You control the TARGET base station. Output ONLY (power_level,offload_flag) as integers, do not output thinking process.\n",
    "\n",
    "    - power_level ∈ [1,{max_p1}]\n",
    "    - offload_flag ∈ {0,1} where 1=offload to neighbor (free), 0=keep local.\n",
    "    - Offloading is FREE (no cost, no energy penalty).\n",
    "    - Coverage: 0=good, 1=fair, 2=poor. Capacity: 0=headroom, 1=maxed.\n",
    "\n",
    "    NEIGHBOR: p={ag2_state[0]}, cov={ag2_state[1]}, cap={ag2_state[2]}, drops={ag2_state[3]}\n",
    "    TARGET:   p={ag1_state[0]}, cov={ag1_state[1]}, cap={ag1_state[2]}, drops={ag1_state[3]}\n",
    "\n",
    "    Priority: Avoid drops > everything. If uncertain, default offload_flag=1.\n",
    "\n",
    "    Hard rules:\n",
    "    - If TARGET cap==1 and NEIGHBOR cap==0 → offload_flag=1.\n",
    "    - If TARGET drops>0 → offload_flag=1 and power_level = min(p+1,{max_p1}).\n",
    "    - If TARGET cap==0 and drops==0 → you MAY reduce power by 1 if still avoids drops.\n",
    "    - Otherwise keep current power.\n",
    "\n",
    "    Return only the tuple: (power_level,offload_flag)\n",
    "    \"\"\"\n",
    "\n",
    "    message = c_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=100,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output = message.content[0].text\n",
    "    print(output)\n",
    "    output = ast.literal_eval(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_llm(df1, df2):\n",
    "    # 43-46 dbm\n",
    "    env1 = CellTowerEnv(MAPPING1, 50)\n",
    "    # 30-38 dbm\n",
    "    env2 = CellTowerEnv(MAPPING2, 30)\n",
    "    \n",
    "    agent1 = SingleAgent(\n",
    "        num_power_level=env1.max_power,\n",
    "        action_size=env1.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag1_qtable.npy'\n",
    "    )\n",
    "    agent2 = SingleAgent(\n",
    "        num_power_level=env2.max_power,\n",
    "        action_size=env2.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag2_qtable.npy'\n",
    "    )\n",
    "\n",
    "    max_steps = len(df1)\n",
    "    for t in range(max_steps):\n",
    "        print(t)\n",
    "        calls1 = df1.iloc[t]['calls']\n",
    "        # print(f'call1: {len(calls1)}')\n",
    "        calls2 = df2.iloc[t]['calls']\n",
    "        # print(f'call2: {len(calls2)}')\n",
    "\n",
    "        # --- stations handle their own calls ---\n",
    "        state1 = env1.get_state()\n",
    "        state2 = env2.get_state()\n",
    "\n",
    "        rl_action1 = agent1.choose_action(state1)\n",
    "        # llm_action1 = gpt_action(state1, env1.max_power, state2, env2.max_power)\n",
    "        llm_action1 = claude_action(state1, env1.max_power, state2, env2.max_power)\n",
    "        decision1 = agent1.make_decision(state1, llm_action1, rl_action1)\n",
    "        rl_action2 = agent2.choose_action(state2)\n",
    "        # llm_action2 = gpt_action(state2, env2.max_power, state1, env1.max_power)\n",
    "        llm_action2 = claude_action(state2, env2.max_power, state1, env1.max_power)\n",
    "        decision2 = agent2.make_decision(state2, llm_action2, rl_action2)\n",
    "\n",
    "        handled1, dropped1= env1.apply_action(decision1, calls1)\n",
    "        handled2, dropped2= env2.apply_action(decision2, calls2)\n",
    "\n",
    "        # --- stations handle handoffs ---\n",
    "        if dropped1 >= 0 and decision1[1] == 1:\n",
    "            added2, failed2 = env2.add_requests(calls1[handled1:])\n",
    "        else:\n",
    "            added2 = 0\n",
    "            failed2 = 0\n",
    "        if dropped2 >= 0 and decision2[1] == 1:\n",
    "            added1, failed1 = env1.add_requests(calls2[handled2:])\n",
    "        else:\n",
    "            added1 = 0\n",
    "            failed1 = 0\n",
    "\n",
    "        # --- update how many requests got handled and dropped ---\n",
    "        handled1 += added1\n",
    "        dropped2 -= added1\n",
    "        handled2 += added2\n",
    "        dropped1 -= added2\n",
    "\n",
    "        # --- compute rewards ---\n",
    "        reward1 = env1.compute_reward(decision1, handled1, dropped1)\n",
    "        reward2 = env2.compute_reward(decision2, handled2, dropped2)\n",
    "\n",
    "        # --- update trusts ---\n",
    "        t_old1, t_new1 = agent1.update_trust(state1, decision1, reward1, llm_action1, rl_action1)\n",
    "        t_old2, t_new2 = agent2.update_trust(state2, decision2, reward2, llm_action2, rl_action2)\n",
    "\n",
    "        llm_ag1.loc[len(llm_ag1)] = [state1, rl_action1, llm_action1, \n",
    "                                decision1, reward1, handled1, dropped1,\n",
    "                                t_old1, t_new1]\n",
    "        llm_ag2.loc[len(llm_ag2)] = [state2, rl_action2, llm_action2, \n",
    "                                decision2, reward2, handled2, dropped2,\n",
    "                                t_old2, t_new2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45517cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/htraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/htraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "testing_llm(df1.iloc[:200], df2.iloc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30936b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agent1 handled: {llm_ag1['handled'].sum()}')\n",
    "print(f'Agent2 handled: {llm_ag2['handled'].sum()}')\n",
    "print(f'Agent1 dropped: {llm_ag1['dropped'].sum()}')\n",
    "print(f'Agent2 dropped: {llm_ag2['dropped'].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885637f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_ag1 = pd.DataFrame(columns=['state', 'action', 'reward', 'handled', 'dropped'])\n",
    "rl_ag2 = pd.DataFrame(columns=['state', 'action', 'reward', 'handled', 'dropped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_rl(df1, df2):\n",
    "    # 43-46 dbm\n",
    "    env1 = CellTowerEnv(MAPPING1, 50)\n",
    "    # 30-38 dbm\n",
    "    env2 = CellTowerEnv(MAPPING2, 30)\n",
    "    \n",
    "    agent1 = SingleAgent(\n",
    "        num_power_level=env1.max_power,\n",
    "        action_size=env1.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag1_qtable.npy'\n",
    "    )\n",
    "    agent2 = SingleAgent(\n",
    "        num_power_level=env2.max_power,\n",
    "        action_size=env2.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag2_qtable.npy'\n",
    "    )\n",
    "\n",
    "    max_steps = len(df1)\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        calls1 = df1.iloc[t]['calls']\n",
    "        calls2 = df2.iloc[t]['calls']\n",
    "\n",
    "        # --- stations handle their own calls ---\n",
    "        state1 = env1.get_state()\n",
    "        action1 = agent1.choose_action(state1)\n",
    "        handled1, dropped1= env1.apply_action(action1, calls1)\n",
    "\n",
    "        state2 = env2.get_state()\n",
    "        action2 = agent2.choose_action(state2)\n",
    "        handled2, dropped2= env2.apply_action(action2, calls2)\n",
    "\n",
    "        # --- stations handle handoffs ---\n",
    "        if dropped1 >= 0 and action1[1] == 1:\n",
    "            added2, failed2 = env2.add_requests(calls1[handled1:])\n",
    "        else:\n",
    "            added2 = 0\n",
    "            failed2 = 0\n",
    "        if dropped2 >= 0 and action2[1] == 1:\n",
    "            added1, failed1 = env1.add_requests(calls2[handled2:])\n",
    "        else:\n",
    "            added1 = 0\n",
    "            failed1 = 0\n",
    "        # --- update how many requests got handled and dropped ---\n",
    "        handled1 += added1\n",
    "        dropped2 -= added1\n",
    "        handled2 += added2\n",
    "        dropped1 -= added2\n",
    "\n",
    "        # --- compute rewards ---\n",
    "        reward1 = env1.compute_reward(action1, handled1, dropped1)\n",
    "        reward2 = env2.compute_reward(action2, handled2, dropped2)\n",
    "\n",
    "        rl_ag1.loc[len(rl_ag1)] = [state1, action1, reward1, handled1, dropped1]\n",
    "        rl_ag2.loc[len(rl_ag2)] = [state2, action2, reward2, handled2, dropped2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/ntraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/ntraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "testing_rl(df1.iloc[800:1000], df2.iloc[800:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d18b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ed9b37",
   "metadata": {},
   "source": [
    "### delayed-reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb02985",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ag1 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])\n",
    "llm_ag2 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96322e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_client = anthropic.Anthropic(api_key='')\n",
    "\n",
    "def claude_action(ag1_state, max_p1, ag2_state, max_p2):\n",
    "    prompt = f\"\"\"\n",
    "    You control the TARGET base station. Output ONLY (power_level,offload_flag) as integers, do not output thinking process.\n",
    "\n",
    "    - power_level ∈ [1,{max_p1}]\n",
    "    - offload_flag ∈ {0,1} where 1=offload to neighbor (free), 0=keep local.\n",
    "    - Offloading is FREE (no cost, no energy penalty).\n",
    "    - Coverage: 0=good, 1=fair, 2=poor. Capacity: 0=headroom, 1=maxed.\n",
    "\n",
    "    NEIGHBOR: p={ag2_state[0]}, cov={ag2_state[1]}, cap={ag2_state[2]}, drops={ag2_state[3]}\n",
    "    TARGET:   p={ag1_state[0]}, cov={ag1_state[1]}, cap={ag1_state[2]}, drops={ag1_state[3]}\n",
    "\n",
    "    Priority: Avoid drops > everything. If uncertain, default offload_flag=1.\n",
    "\n",
    "    Hard rules:\n",
    "    - If TARGET cap==1 and NEIGHBOR cap==0 → offload_flag=1.\n",
    "    - If TARGET drops>0 → offload_flag=1 and power_level = min(p+1,{max_p1}).\n",
    "    - If TARGET cap==0 and drops==0 → you MAY reduce power by 1 if still avoids drops.\n",
    "    - Otherwise keep current power.\n",
    "\n",
    "    Return only the tuple: (power_level,offload_flag)\n",
    "    \"\"\"\n",
    "\n",
    "    message = c_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=100,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output = message.content[0].text\n",
    "    print(f'LLM action: {output}')\n",
    "    output = ast.literal_eval(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def llm_reward(ag1_hist, ag2_hist):\n",
    "    prompt = f'''\n",
    "        You are evaluating the performance of two cellular base stations (agent_1 and agent_2).\n",
    "\n",
    "        The agents operate for several time steps. Each agent's record for a time step contains:\n",
    "        - \"state\": a tuple (power_level, cover_index, capacity, unserved_last_time)\n",
    "            * power_level is an integer, higher values mean more transmit power (more energy use, but more coverage).\n",
    "            * coverage quality: 0 - good, 1 - fair, 2 - poor.\n",
    "            * capacity: whether or not the station had reach its maximum capacity: 0 - no, 1 - yes.\n",
    "            * unserved_last_time: how many requests were not handled due to the capacity limit\n",
    "        - \"action\": a tuple (power_level, handoff_decision)\n",
    "            * power_level is an integer, higher values mean more transmit power (more energy use, but more coverage).\n",
    "            * handoff_decision is 0 or 1, where 1 means the agent tries to hand off unserved requests to another station.\n",
    "        - \"reward\": the environment's immediate reward at this step (already computed).\n",
    "        - \"total\": total number of user requests received at this step.\n",
    "        - \"handled\": how many of those requests this agent successfully served.\n",
    "\n",
    "        You will receive two separate lists:\n",
    "        - One list for agent_0 (its records across k steps)\n",
    "        - One list for agent_1 (its records across the same k steps)\n",
    "\n",
    "        Task:\n",
    "        - Evaluate each agent's performance *individually* over the interval.\n",
    "        - Consider throughput (handled/total), fairness between agents, and efficiency \n",
    "        (avoid rewarding unnecessary power use).\n",
    "        - Based on this, assign a **correctional reward** for each agent between -1.0 (very poor) \n",
    "        and +1.0 (excellent). The reward can be decimals such as 0.37 or -0.15.\n",
    "\n",
    "        Agent records:\n",
    "        - agent_1: {ag1_hist}\n",
    "        - agent_2: {ag2_hist}\n",
    "        \n",
    "        Return ONLY valid JSON with the following structure:\n",
    "        {{\n",
    "        \"agent_1\": numeric score,\n",
    "        \"agent_2\": numeric score\n",
    "        }}. \n",
    "        Do not include thinking process.\n",
    "        '''\n",
    "    \n",
    "    message = c_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=100,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output = message.content[0].text.strip(\" `\")\n",
    "    match = re.search(r\"\\{[\\s\\S]*?\\}\", output)\n",
    "    try:\n",
    "        result = json.loads(match.group(0))\n",
    "        reward1 = float(result[\"agent_1\"])\n",
    "        reward2 = float(result[\"agent_2\"])\n",
    "        print(f'Delayed-rewards: {reward1}, {reward2}')\n",
    "        return reward1, reward2\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse LLM response: {match.group(1)}\") from e\n",
    "    \n",
    "\n",
    "def split_reward(total_reward, records):\n",
    "    k = len(records)\n",
    "    if k == 0:\n",
    "        return []\n",
    "\n",
    "    handled_sum = sum(max(0, int(r.get(\"handled\", 0))) for r in records)\n",
    "    uniform = total_reward / k\n",
    "\n",
    "    if handled_sum == 0:\n",
    "        return [uniform] * k\n",
    "\n",
    "    return [\n",
    "        0.5 * uniform + 0.5 * total_reward * (r.get(\"handled\", 0) / handled_sum)\n",
    "        for r in records\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a321dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_delayed_reward(df1, df2, log1, log2):\n",
    "    # 43-46 dbm\n",
    "    env1 = CellTowerEnv(MAPPING1, 50)\n",
    "    # 30-38 dbm\n",
    "    env2 = CellTowerEnv(MAPPING2, 30)\n",
    "    \n",
    "    agent1 = SingleAgent(\n",
    "        num_power_level=env1.max_power,\n",
    "        action_size=env1.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag1_qtable.npy'\n",
    "    )\n",
    "    agent2 = SingleAgent(\n",
    "        num_power_level=env2.max_power,\n",
    "        action_size=env2.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag2_qtable.npy'\n",
    "    )\n",
    "\n",
    "    ag1_hist = []\n",
    "    ag2_hist = []\n",
    "\n",
    "    max_steps = len(df1)\n",
    "    for t in range(max_steps):\n",
    "        print(t)\n",
    "        calls1 = df1.iloc[t]['calls']\n",
    "        calls2 = df2.iloc[t]['calls']\n",
    "\n",
    "        # --- stations handle their own calls ---\n",
    "        state1 = env1.get_state()\n",
    "        state2 = env2.get_state()\n",
    "\n",
    "        rl_action1 = agent1.choose_action(state1)\n",
    "        llm_action1 = claude_action(state1, env1.max_power, state2, env2.max_power)\n",
    "        decision1 = agent1.make_decision(state1, llm_action1, rl_action1)\n",
    "        rl_action2 = agent2.choose_action(state2)\n",
    "        llm_action2 = claude_action(state2, env2.max_power, state1, env1.max_power)\n",
    "        decision2 = agent2.make_decision(state2, llm_action2, rl_action2)\n",
    "\n",
    "        handled1, dropped1= env1.apply_action(decision1, calls1)\n",
    "        handled2, dropped2= env2.apply_action(decision2, calls2)\n",
    "\n",
    "        # --- stations handle handoffs ---\n",
    "        if dropped1 >= 0 and decision1[1] == 1:\n",
    "            added2, failed2 = env2.add_requests(calls1[handled1:])\n",
    "        else:\n",
    "            added2 = 0\n",
    "            failed2 = 0\n",
    "        if dropped2 >= 0 and decision2[1] == 1:\n",
    "            added1, failed1 = env1.add_requests(calls2[handled2:])\n",
    "        else:\n",
    "            added1 = 0\n",
    "            failed1 = 0\n",
    "\n",
    "        # --- update how many requests got handled and dropped ---\n",
    "        handled1 += added1\n",
    "        dropped2 -= added1\n",
    "        handled2 += added2\n",
    "        dropped1 -= added2\n",
    "\n",
    "        # --- compute rewards ---\n",
    "        reward1 = env1.compute_reward(decision1, handled1, dropped1)\n",
    "        reward2 = env2.compute_reward(decision2, handled2, dropped2)\n",
    "\n",
    "        # --- update trusts ---\n",
    "        t_old1, t_new1 = agent1.update_trust(state1, decision1, reward1, llm_action1, rl_action1)\n",
    "        t_old2, t_new2 = agent2.update_trust(state2, decision2, reward2, llm_action2, rl_action2)\n",
    "\n",
    "        # --- log ---\n",
    "        llm_ag1.loc[len(llm_ag1)] = [state1, rl_action1, llm_action1, \n",
    "                                decision1, reward1, handled1, dropped1,\n",
    "                                t_old1, t_new1]\n",
    "        llm_ag2.loc[len(llm_ag2)] = [state2, rl_action2, llm_action2, \n",
    "                                decision2, reward2, handled2, dropped2,\n",
    "                                t_old2, t_new2]\n",
    "        \n",
    "        # --- delayed rewards ---\n",
    "        ag1_hist.append(dict(state=state1, action=decision1, reward=reward1, total=calls1, handled=handled1))\n",
    "        ag2_hist.append(dict(state=state2, action=decision2, reward=reward2, total=calls2, handled=handled2))\n",
    "\n",
    "        if len(ag1_hist) == 3:\n",
    "            d_reward1, d_reward2 = llm_reward(ag1_hist, ag2_hist)\n",
    "            split1 = split_reward(d_reward1, ag1_hist)\n",
    "            agent1.apply_delayed_reward(ag1_hist, split1, 0.15)\n",
    "            split2 = split_reward(d_reward2, ag2_hist)\n",
    "            agent2.apply_delayed_reward(ag2_hist, split2, 0.15)\n",
    "\n",
    "            log1.loc[len(log1)] = [ag1_hist, d_reward1, split1]\n",
    "            log2.loc[len(log1)] = [ag2_hist, d_reward2, split2]\n",
    "            ag1_hist = []\n",
    "            ag2_hist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d06671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/ltraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/ltraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "log1 = pd.DataFrame(columns=['Records', 'Rewards', 'Splits'])\n",
    "log2 = pd.DataFrame(columns=['Records', 'Rewards', 'Splits'])\n",
    "with_delayed_reward(df1.iloc[:200], df2.iloc[:200], log1, log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f60f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agent1 handled: {llm_ag1['handled'].sum()}')\n",
    "print(f'Agent2 handled: {llm_ag2['handled'].sum()}')\n",
    "print(f'Agent1 dropped: {llm_ag1['dropped'].sum()}')\n",
    "print(f'Agent2 dropped: {llm_ag2['dropped'].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f637af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26d0d5b6",
   "metadata": {},
   "source": [
    "### adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_llm(df1, df2):\n",
    "    # 43-46 dbm\n",
    "    env1 = CellTowerEnv(MAPPING1, 50)\n",
    "    # 30-38 dbm\n",
    "    env2 = CellTowerEnv(MAPPING2, 30)\n",
    "    \n",
    "    agent1 = SingleAgent(\n",
    "        num_power_level=env1.max_power,\n",
    "        action_size=env1.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag1_qtable.npy'\n",
    "    )\n",
    "    agent2 = SingleAgent(\n",
    "        num_power_level=env2.max_power,\n",
    "        action_size=env2.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag2_qtable.npy'\n",
    "    )\n",
    "\n",
    "    max_steps = len(df1)\n",
    "    for t in range(max_steps):\n",
    "        print(t)\n",
    "        calls1 = df1.iloc[t]['calls']\n",
    "        calls2 = df2.iloc[t]['calls']\n",
    "\n",
    "        # --- stations handle their own calls ---\n",
    "        state1 = env1.get_state()\n",
    "        state2 = env2.get_state()\n",
    "\n",
    "        rl_action1 = agent1.choose_action(state1)\n",
    "        llm_action1 = claude_action(state1, env1.max_power, state2, env2.max_power)\n",
    "        decision1 = agent1.make_decision(state1, llm_action1, rl_action1)\n",
    "        rl_action2 = agent2.choose_action(state2)\n",
    "        llm_action2 = claude_action(state2, env2.max_power, state1, env1.max_power)\n",
    "        decision2 = agent2.make_decision(state2, llm_action2, rl_action2)\n",
    "\n",
    "        handled1, dropped1= env1.apply_action(decision1, calls1)\n",
    "        handled2, dropped2= env2.apply_action(decision2, calls2)\n",
    "\n",
    "        # --- stations handle handoffs ---\n",
    "        if dropped1 >= 0 and decision1[1] == 1:\n",
    "            added2, failed2 = env2.add_requests(calls1[handled1:])\n",
    "        else:\n",
    "            added2 = 0\n",
    "            failed2 = 0\n",
    "        if dropped2 >= 0 and decision2[1] == 1:\n",
    "            added1, failed1 = env1.add_requests(calls2[handled2:])\n",
    "        else:\n",
    "            added1 = 0\n",
    "            failed1 = 0\n",
    "\n",
    "        # --- update how many requests got handled and dropped ---\n",
    "        handled1 += added1\n",
    "        dropped2 -= added1\n",
    "        handled2 += added2\n",
    "        dropped1 -= added2\n",
    "\n",
    "        # --- compute rewards ---\n",
    "        reward1 = env1.compute_reward(decision1, handled1, dropped1)\n",
    "        reward2 = env2.compute_reward(decision2, handled2, dropped2)\n",
    "\n",
    "        # --- Q-learning update ---\n",
    "        agent1.learn(state1, decision1, reward1, env1.get_state())\n",
    "        agent2.learn(state2, decision2, reward2, env2.get_state())\n",
    "\n",
    "        # --- update trusts ---\n",
    "        t_old1, t_new1 = agent1.update_trust(state1, decision1, reward1, llm_action1, rl_action1)\n",
    "        t_old2, t_new2 = agent2.update_trust(state2, decision2, reward2, llm_action2, rl_action2)\n",
    "\n",
    "        llm_ag1.loc[len(llm_ag1)] = [state1, rl_action1, llm_action1, \n",
    "                                decision1, reward1, handled1, dropped1,\n",
    "                                t_old1, t_new1]\n",
    "        llm_ag2.loc[len(llm_ag2)] = [state2, rl_action2, llm_action2, \n",
    "                                decision2, reward2, handled2, dropped2,\n",
    "                                t_old2, t_new2]\n",
    "    return agent1, agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0916b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ag1 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])\n",
    "llm_ag2 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04064d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/htraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/htraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "llm1, llm2 = adapt_llm(df1.iloc[:200], df2.iloc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agent1 handled: {llm_ag1['handled'].sum()}')\n",
    "print(f'Agent2 handled: {llm_ag2['handled'].sum()}')\n",
    "print(f'Agent1 dropped: {llm_ag1['dropped'].sum()}')\n",
    "print(f'Agent2 dropped: {llm_ag2['dropped'].sum()}')\n",
    "print('\\n')\n",
    "print(f'Avg Reward1: {llm_ag1['reward'].mean()}')\n",
    "print(f'Avg Reward2: {llm_ag2['reward'].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_rl(df1, df2):\n",
    "    # 43-46 dbm\n",
    "    env1 = CellTowerEnv(MAPPING1, 50)\n",
    "    # 30-38 dbm\n",
    "    env2 = CellTowerEnv(MAPPING2, 30)\n",
    "    \n",
    "    agent1 = SingleAgent(\n",
    "        num_power_level=env1.max_power,\n",
    "        action_size=env1.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag1_qtable.npy'\n",
    "    )\n",
    "    agent2 = SingleAgent(\n",
    "        num_power_level=env2.max_power,\n",
    "        action_size=env2.action_space_size,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        table='ag2_qtable.npy'\n",
    "    )\n",
    "\n",
    "    max_steps = len(df1)\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        calls1 = df1.iloc[t]['calls']\n",
    "        calls2 = df2.iloc[t]['calls']\n",
    "\n",
    "        # --- stations handle their own calls ---\n",
    "        state1 = env1.get_state()\n",
    "        action1 = agent1.choose_action(state1)\n",
    "        handled1, dropped1= env1.apply_action(action1, calls1)\n",
    "\n",
    "        state2 = env2.get_state()\n",
    "        action2 = agent2.choose_action(state2)\n",
    "        handled2, dropped2= env2.apply_action(action2, calls2)\n",
    "\n",
    "        # --- stations handle handoffs ---\n",
    "        if dropped1 >= 0 and action1[1] == 1:\n",
    "            added2, failed2 = env2.add_requests(calls1[handled1:])\n",
    "        else:\n",
    "            added2 = 0\n",
    "            failed2 = 0\n",
    "        if dropped2 >= 0 and action2[1] == 1:\n",
    "            added1, failed1 = env1.add_requests(calls2[handled2:])\n",
    "        else:\n",
    "            added1 = 0\n",
    "            failed1 = 0\n",
    "        # --- update how many requests got handled and dropped ---\n",
    "        handled1 += added1\n",
    "        dropped2 -= added1\n",
    "        handled2 += added2\n",
    "        dropped1 -= added2\n",
    "\n",
    "        # --- compute rewards ---\n",
    "        reward1 = env1.compute_reward(action1, handled1, dropped1)\n",
    "        reward2 = env2.compute_reward(action2, handled2, dropped2)\n",
    "\n",
    "        # --- Q-learning update ---\n",
    "        agent1.learn(state1, action1, reward1, env1.get_state())\n",
    "        agent2.learn(state2, action2, reward2, env2.get_state())\n",
    "\n",
    "        rl_ag1.loc[len(rl_ag1)] = [state1, action1, reward1, handled1, dropped1]\n",
    "        rl_ag2.loc[len(rl_ag2)] = [state2, action2, reward2, handled2, dropped2]\n",
    "    return agent1, agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_ag1 = pd.DataFrame(columns=['state', 'action', 'reward', 'handled', 'dropped'])\n",
    "rl_ag2 = pd.DataFrame(columns=['state', 'action', 'reward', 'handled', 'dropped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288df991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/htraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/htraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "rl1, rl2 = adapt_rl(df1.iloc[:200], df2.iloc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b648a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agent1 handled: {rl_ag1['handled'].sum()}')\n",
    "print(f'Agent2 handled: {rl_ag2['handled'].sum()}')\n",
    "print(f'Agent1 dropped: {rl_ag1['dropped'].sum()}')\n",
    "print(f'Agent2 dropped: {rl_ag2['dropped'].sum()}')\n",
    "print('\\n')\n",
    "print(f'Avg Reward1: {rl_ag1['reward'].mean()}')\n",
    "print(f'Avg Reward2: {rl_ag2['reward'].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(df1, col1, df2, col2):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(df1.index, df1[col1], label=f\"LLM\")\n",
    "    plt.plot(df2.index, df2[col2], label=f\"RL\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(llm_ag2, 'reward', rl_ag2, 'reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ag1 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])\n",
    "llm_ag2 = pd.DataFrame(columns=['state', 'rl_action', 'llm_action', \n",
    "                                'decision', 'reward', 'handled', 'dropped',\n",
    "                                'trust_before', 'trust_after'])\n",
    "\n",
    "df1 = pd.read_csv('data/ltraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/ltraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "llm1, llm2 = adapt_llm(df1.iloc[:200], df2.iloc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agent1 handled: {llm_ag1['handled'].sum()}')\n",
    "print(f'Agent2 handled: {llm_ag2['handled'].sum()}')\n",
    "print(f'Agent1 dropped: {llm_ag1['dropped'].sum()}')\n",
    "print(f'Agent2 dropped: {llm_ag2['dropped'].sum()}')\n",
    "print('\\n')\n",
    "print(f'Avg Reward1: {llm_ag1['reward'].mean()}')\n",
    "print(f'Avg Reward2: {llm_ag2['reward'].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_ag1 = pd.DataFrame(columns=['state', 'action', 'reward', 'handled', 'dropped'])\n",
    "rl_ag2 = pd.DataFrame(columns=['state', 'action', 'reward', 'handled', 'dropped'])\n",
    "\n",
    "df1 = pd.read_csv('data/ltraffic_station1.csv')\n",
    "df1['calls'] = df1['calls'].apply(ast.literal_eval)\n",
    "df2 = pd.read_csv('data/ltraffic_station2.csv')\n",
    "df2['calls'] = df2['calls'].apply(ast.literal_eval)\n",
    "rl1, rl2 = adapt_rl(df1.iloc[:200], df2.iloc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529577d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agent1 handled: {rl_ag1['handled'].sum()}')\n",
    "print(f'Agent2 handled: {rl_ag2['handled'].sum()}')\n",
    "print(f'Agent1 dropped: {rl_ag1['dropped'].sum()}')\n",
    "print(f'Agent2 dropped: {rl_ag2['dropped'].sum()}')\n",
    "print('\\n')\n",
    "print(f'Avg Reward1: {rl_ag1['reward'].mean()}')\n",
    "print(f'Avg Reward2: {rl_ag2['reward'].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_ag1.to_csv('learning/l_rl_ag1.csv', index=False)\n",
    "rl_ag2.to_csv('learning/l_rl_ag2.csv', index=False)\n",
    "\n",
    "np.save('learning/l_rl_ag1.npy', rl1.Q)\n",
    "np.save('learning/l_rl_ag2.npy', rl2.Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
